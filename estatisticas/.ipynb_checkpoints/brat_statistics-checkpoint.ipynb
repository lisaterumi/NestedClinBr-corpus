{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clk8Dp2h-0QX"
   },
   "source": [
    "# Statistics\n",
    "This notebooks allows you to obtain some statistics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MDZlCuyS2jxg",
    "outputId": "bdc8f468-b439-4a0c-e67f-55a4213e0674"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyforest==0.1.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/7a/2280448ba4202604eb3f9e23d9a4fd0ca1473d31aca0a90fdb5f31dd902c/pyforest-0.1.1.tar.gz (3.4MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4MB 6.5MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyforest\n",
      "  Building wheel for pyforest (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyforest: filename=pyforest-0.1.1-py2.py3-none-any.whl size=9213 sha256=e93e1b16570bceb4ee5e560d59e406fd016e03cef9e32bcd258082541327cd2d\n",
      "  Stored in directory: /root/.cache/pip/wheels/77/f9/78/51500678d6ce472b574216a40cba6c81d1766ee7cc838cce3c\n",
      "Successfully built pyforest\n",
      "Installing collected packages: pyforest\n",
      "Successfully installed pyforest-0.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyforest==0.1.1\n",
    "#https://pypi.org/project/pyforest/0.1.1/\n",
    "import pyforest  #With this library, you won't need to import more packages!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uLgJ6C4H2C_J",
    "outputId": "86cbdc95-f19a-4200-f7b2-db07bd517ed6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n",
      "training: drive/My Drive/Colab Notebooks/nlp4rareNER/data/goldstandard/train/\n",
      "dev: drive/My Drive/Colab Notebooks/nlp4rareNER/data/goldstandard/dev/\n",
      "test: drive/My Drive/Colab Notebooks/nlp4rareNER/data/goldstandard/test/\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount(\"/content/drive/\")\n",
    "\n",
    "#root='drive/My Drive/Colab Notebooks/nlp4rareNER/'\n",
    "#path=root+'data/goldstandard/'\n",
    "\n",
    "path = '../corpus/brat/'\n",
    "\n",
    "train_path=path+'train/'\n",
    "dev_path=path+'dev/'\n",
    "test_path=path+'test/'\n",
    "\n",
    "print(\"training:\", train_path)\n",
    "print(\"dev:\", dev_path)\n",
    "print(\"test:\", test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtMcXTN02VEQ"
   },
   "source": [
    "## Number of documents, sentences and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "he3lPxAE19gg",
    "outputId": "0d57d31e-75e2-4135-8522-fc9a1e4219fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents in training: 729\n",
      "number of documents in development: 104\n",
      "number of documents in test: 209\n",
      "total of documents: 1042\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "size_train=len(glob.glob(train_path+'*.txt'))\n",
    "print('number of documents in training:',size_train)\n",
    "size_dev=len(glob.glob(dev_path+'*.txt'))\n",
    "print('number of documents in development:',size_dev)\n",
    "size_test=len(glob.glob(test_path+'*.txt'))\n",
    "print('number of documents in test:',size_test)\n",
    "total=size_train+size_dev+size_test\n",
    "print('total of documents:',total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n68iJmww3aG-"
   },
   "source": [
    "We will install Spacy, a library NLP, that helps us to tokenize the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PYnegGK92aSw",
    "outputId": "feb78a92-b1ce-46a0-8c2c-9cb7cd92973e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm #install spacy and download the model en_core_web_sm\n",
    "import spacy #NLP library for sentence segmentation and tokenization\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2MsOgZcGsak"
   },
   "outputs": [],
   "source": [
    "def getText(path_file):\n",
    "    \"\"\"returns the text of the file path_file\"\"\"\n",
    "    f=open(path_file,'r')\n",
    "    text=f.read()\n",
    "    f.close()\n",
    "    return text\n",
    "\n",
    "def countTokens(path):\n",
    "    \"\"\"This functions takes a directory path where a dataset is stored, g\n",
    "    gets its txt files to parse them  by using Spacy \n",
    "    for obtaining the number of tokens and sentences in\n",
    "    the dataset\"\"\"\n",
    "    \n",
    "    total_sentences=0\n",
    "    total_tokens=0\n",
    "    files=glob.glob(path+'*.txt')\n",
    "    #print(path,len(files))\n",
    "    \n",
    "    for i,file_text in enumerate(files):\n",
    "        text=getText(file_text)\n",
    "        #print(file_text,text)\n",
    "\n",
    "        doc = nlp(text)\n",
    "        total_sentences+=len(list(doc.sents))\n",
    "        total_tokens+=len(doc)\n",
    "\n",
    "    return total_sentences, total_tokens\n",
    "\n",
    "sentences_train, tokens_train = countTokens(train_path)\n",
    "sentences_dev, tokens_dev = countTokens(dev_path)\n",
    "sentences_test, tokens_test = countTokens(test_path)\n",
    "\n",
    "print('Sentences and tokens in training: ', sentences_train, tokens_train)\n",
    "print('Sentences and tokens in dev: ', sentences_dev, tokens_dev)\n",
    "print('Sentences and tokens in text: ', sentences_test, tokens_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVhCbnN8DQEX"
   },
   "source": [
    "## Number of entities\n",
    "\n",
    "We now obtain the number of entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G3Tp5U7yALUp",
    "outputId": "6b2bc29d-a16b-47db-bd08-f2372ac83389"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities,Diseases,Rare Diseases, Skin Rare Diseases, Symptoms, Signs\n",
      "8774 0 3157 451 318 3742\n",
      "1228 0 480 45 24 528\n",
      "2478 0 942 146 53 1061\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def getAnnotations(path_file,allTypes=False):\n",
    "    \"\"\"returns a dataframe with its annotations\"\"\"\n",
    "    annotations=[]\n",
    "    try:  \n",
    "        f=open(path_file,'r')\n",
    "        annotations = f.readlines()       \n",
    "        f.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    num_entities=0\n",
    "    total_dis=0\n",
    "    total_raredis=0\n",
    "    total_skin=0\n",
    "    total_sym=0\n",
    "    total_sign=0        \n",
    "\n",
    "\n",
    "    for ann in annotations:\n",
    "        if ann.startswith('T'):\n",
    "            idEnt=ann[:ann.index('\\t')]\n",
    "            typeEnt=ann[ann.index('\\t')+1:ann.index(' ')]\n",
    "            num_entities+=1\n",
    "\n",
    "            if typeEnt=='DISEASE':\n",
    "                if allTypes:\n",
    "                    total_dis+=1\n",
    "                else:\n",
    "                    num_entities-=1\n",
    "            elif typeEnt=='RAREDISEASE':\n",
    "                total_raredis+=1\n",
    "            elif typeEnt=='SKINRAREDISEASE':\n",
    "                total_skin+=1\n",
    "            elif typeEnt=='SYMPTOM':\n",
    "                total_sym+=1\n",
    "            elif typeEnt=='SIGN':\n",
    "                total_sign+=1\n",
    "            else:\n",
    "                #if allTypes==False\n",
    "                pass\n",
    "\n",
    "    return num_entities, total_dis, total_raredis, total_skin, total_sym, total_sign\n",
    "\n",
    "\n",
    "def countEntities(path):\n",
    "    total_entities=0\n",
    "    total_dis=0\n",
    "    total_rare=0\n",
    "    total_skin=0\n",
    "    total_sym=0\n",
    "    total_sign=0\n",
    "\n",
    "    files=glob.glob(path+'*.ann')\n",
    "    #print(path,len(files))\n",
    "    \n",
    "    for i,file_path in enumerate(files):\n",
    "        ne, td, tr, tsk, tsy, tsi=getAnnotations(file_path)\n",
    "        total_entities+=ne\n",
    "        total_dis+=td\n",
    "        total_rare+=tr\n",
    "        total_skin+=tsk\n",
    "        total_sym+=tsy\n",
    "        total_sign+=tsi\n",
    "\n",
    "    return total_entities, total_dis, total_rare, total_skin, total_sym, total_sign\n",
    "\n",
    "\n",
    "\n",
    "print('Entities,Diseases,Rare Diseases, Skin Rare Diseases, Symptoms, Signs')\n",
    "total_entities, total_dis, total_rare, total_skin, total_sym, total_sign= countEntities(train_path)\n",
    "print(total_entities, total_dis, total_rare, total_skin, total_sym, total_sign)\n",
    "total_entities, total_dis, total_rare, total_skin, total_sym, total_sign = countEntities(dev_path)\n",
    "print(total_entities, total_dis, total_rare, total_skin, total_sym, total_sign)\n",
    "\n",
    "total_entities, total_dis, total_rare, total_skin, total_sym, total_sign = countEntities(test_path)\n",
    "print(total_entities, total_dis, total_rare, total_skin, total_sym, total_sign)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "brat_statistics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
