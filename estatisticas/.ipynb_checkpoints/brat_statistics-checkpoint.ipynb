{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clk8Dp2h-0QX"
   },
   "source": [
    "# Statistics\n",
    "This notebooks allows you to obtain some statistics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MDZlCuyS2jxg",
    "outputId": "bdc8f468-b439-4a0c-e67f-55a4213e0674"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyforest==0.1.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/7a/2280448ba4202604eb3f9e23d9a4fd0ca1473d31aca0a90fdb5f31dd902c/pyforest-0.1.1.tar.gz (3.4MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4MB 6.5MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyforest\n",
      "  Building wheel for pyforest (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyforest: filename=pyforest-0.1.1-py2.py3-none-any.whl size=9213 sha256=e93e1b16570bceb4ee5e560d59e406fd016e03cef9e32bcd258082541327cd2d\n",
      "  Stored in directory: /root/.cache/pip/wheels/77/f9/78/51500678d6ce472b574216a40cba6c81d1766ee7cc838cce3c\n",
      "Successfully built pyforest\n",
      "Installing collected packages: pyforest\n",
      "Successfully installed pyforest-0.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyforest==0.1.1\n",
    "#https://pypi.org/project/pyforest/0.1.1/\n",
    "import pyforest  #With this library, you won't need to import more packages!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uLgJ6C4H2C_J",
    "outputId": "86cbdc95-f19a-4200-f7b2-db07bd517ed6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: ../corpus/brat/train/\n",
      "test: ../corpus/brat/test/\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount(\"/content/drive/\")\n",
    "\n",
    "#root='drive/My Drive/Colab Notebooks/nlp4rareNER/'\n",
    "#path=root+'data/goldstandard/'\n",
    "\n",
    "path = '../corpus/brat/'\n",
    "\n",
    "train_path=path+'train/'\n",
    "test_path=path+'test/'\n",
    "\n",
    "print(\"training:\", train_path)\n",
    "print(\"test:\", test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtMcXTN02VEQ"
   },
   "source": [
    "## Number of documents, sentences and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "he3lPxAE19gg",
    "outputId": "0d57d31e-75e2-4135-8522-fc9a1e4219fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents in training: 100\n",
      "number of documents in test: 26\n",
      "total of documents: 126\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "size_train=len(glob.glob(train_path+'*.txt'))\n",
    "print('number of documents in training:',size_train)\n",
    "size_test=len(glob.glob(test_path+'*.txt'))\n",
    "print('number of documents in test:',size_test)\n",
    "total=size_train+size_test\n",
    "print('total of documents:',total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n68iJmww3aG-"
   },
   "source": [
    "We will install Spacy, a library NLP, that helps us to tokenize the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PYnegGK92aSw",
    "outputId": "feb78a92-b1ce-46a0-8c2c-9cb7cd92973e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-13 09:03:20.084669: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n",
      "2022-08-13 09:03:20.084702: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-08-13 09:03:21.976358: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
      "2022-08-13 09:03:21.976392: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-13 09:03:21.980304: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: TerumiNB\n",
      "2022-08-13 09:03:21.980448: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: TerumiNB\n",
      "ERROR: Invalid requirement: '#install'\n",
      "WARNING: You are using pip version 20.3.3; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\lisat\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt-core-news-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.4.0/pt_core_news_sm-3.4.0-py3-none-any.whl (13.0 MB)\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from pt-core-news-sm==3.4.0) (3.4.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (4.62.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.0.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.11.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (8.1.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.8.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.0.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.19.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.0.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.3.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.25.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.5.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.4.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (51.1.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.9.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.7.4.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2020.6.20)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.7.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.4.4)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\lisat\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.1.1)\n",
      "Installing collected packages: pt-core-news-sm\n",
      "  Attempting uninstall: pt-core-news-sm\n",
      "    Found existing installation: pt-core-news-sm 3.0.0\n",
      "    Uninstalling pt-core-news-sm-3.0.0:\n",
      "      Successfully uninstalled pt-core-news-sm-3.0.0\n",
      "Successfully installed pt-core-news-sm-3.4.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-13 09:03:27.371995: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n",
      "2022-08-13 09:03:27.372028: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-08-13 09:03:29.172040: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
      "2022-08-13 09:03:29.172071: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-13 09:03:29.175607: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: TerumiNB\n",
      "2022-08-13 09:03:29.175728: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: TerumiNB\n",
      "WARNING: You are using pip version 20.3.3; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\lisat\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm #install spacy and download the model en_core_web_sm\n",
    "import spacy #NLP library for sentence segmentation and tokenization\n",
    "#nlp = spacy.load(\"pt_core_news_lg\")\n",
    "!python -m spacy download pt_core_news_sm\n",
    "nlp = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "h2MsOgZcGsak"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences and tokens in training:  2273 17154\n",
      "Sentences and tokens in text:  693 5310\n"
     ]
    }
   ],
   "source": [
    "def getText(path_file):\n",
    "    \"\"\"returns the text of the file path_file\"\"\"\n",
    "    f=open(path_file,'r', encoding='utf-8')\n",
    "    text=f.read()\n",
    "    f.close()\n",
    "    return text\n",
    "\n",
    "def countTokens(path):\n",
    "    \"\"\"This functions takes a directory path where a dataset is stored, g\n",
    "    gets its txt files to parse them  by using Spacy \n",
    "    for obtaining the number of tokens and sentences in\n",
    "    the dataset\"\"\"\n",
    "    \n",
    "    total_sentences=0\n",
    "    total_tokens=0\n",
    "    files=glob.glob(path+'*.txt')\n",
    "    #print(path,len(files))\n",
    "    \n",
    "    for i,file_text in enumerate(files):\n",
    "        text=getText(file_text)\n",
    "        #print(file_text,text)\n",
    "\n",
    "        doc = nlp(text)\n",
    "        total_sentences+=len(list(doc.sents))\n",
    "        total_tokens+=len(doc)\n",
    "\n",
    "    return total_sentences, total_tokens\n",
    "\n",
    "sentences_train, tokens_train = countTokens(train_path)\n",
    "sentences_test, tokens_test = countTokens(test_path)\n",
    "\n",
    "print('Sentences and tokens in training: ', sentences_train, tokens_train)\n",
    "print('Sentences and tokens in text: ', sentences_test, tokens_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVhCbnN8DQEX"
   },
   "source": [
    "## Number of entities\n",
    "\n",
    "We now obtain the number of entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G3Tp5U7yALUp",
    "outputId": "6b2bc29d-a16b-47db-bd08-f2372ac83389"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities,Problema,Teste, Tratamento, Anatomia\n",
      "2188 780 523 528 357\n",
      "651 222 150 140 139\n",
      "\n",
      "--Descontinuas--\n",
      "\n",
      "Entities,Problema,Teste, Tratamento, Anatomia\n",
      "2188 780 523 528 357\n",
      "651 222 150 140 139\n"
     ]
    }
   ],
   "source": [
    "def getAnnotations(path_file,allTypes=False):\n",
    "    \"\"\"returns a dataframe with its annotations\"\"\"\n",
    "    annotations=[]\n",
    "    try:  \n",
    "        f=open(path_file,'r')\n",
    "        annotations = f.readlines()       \n",
    "        f.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    num_entities=0\n",
    "    total_problema=0\n",
    "    total_teste=0\n",
    "    total_tratamento=0\n",
    "    total_anatomia=0\n",
    "\n",
    "    for ann in annotations:\n",
    "        if ann.startswith('T'):\n",
    "            idEnt=ann[:ann.index('\\t')]\n",
    "            typeEnt=ann[ann.index('\\t')+1:ann.index(' ')]\n",
    "            num_entities+=1\n",
    "\n",
    "            if typeEnt=='Problema':\n",
    "                if not allTypes:\n",
    "                    total_problema+=1\n",
    "                else:\n",
    "                    num_entities-=1\n",
    "            elif typeEnt=='Teste':\n",
    "                total_teste+=1\n",
    "            elif typeEnt=='Tratamento':\n",
    "                total_tratamento+=1\n",
    "            elif typeEnt=='Anatomia':\n",
    "                total_anatomia+=1\n",
    "            else:\n",
    "                #if allTypes==False\n",
    "                print('ERRO, typeEnt:', typeEnt)\n",
    "                pass\n",
    "\n",
    "    return num_entities, total_problema, total_teste, total_tratamento, total_anatomia\n",
    "\n",
    "def getAnnotationsDescontinuas(path_file,allTypes=False):\n",
    "    \"\"\"returns a dataframe with its annotations\"\"\"\n",
    "    annotations=[]\n",
    "    try:  \n",
    "        f=open(path_file,'r')\n",
    "        annotations = f.readlines()       \n",
    "        f.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    num_entities=0\n",
    "    total_problema=0\n",
    "    total_teste=0\n",
    "    total_tratamento=0\n",
    "    total_anatomia=0\n",
    "\n",
    "    for ann in annotations:\n",
    "        if ann.startswith('T'):\n",
    "            idEnt=ann[:ann.index('\\t')]\n",
    "            typeEnt=ann[ann.index('\\t')+1:ann.index(' ')]\n",
    "            listaAnn = ann.split('\\t')\n",
    "            indices=listaAnn[1]\n",
    "            if ';' in indices:\n",
    "                # ex T10\tProblema 244 252;283 306\tdispneia mdoeardos-leves esforço\n",
    "                num_entities+=1\n",
    "                if typeEnt=='Problema':\n",
    "                    total_problema+=1\n",
    "                elif typeEnt=='Teste':\n",
    "                    total_teste+=1\n",
    "                elif typeEnt=='Tratamento':\n",
    "                    total_tratamento+=1\n",
    "                elif typeEnt=='Anatomia':\n",
    "                    total_anatomia+=1\n",
    "                else:\n",
    "                    #if allTypes==False\n",
    "                    print('ERRO, typeEnt:', typeEnt)\n",
    "                    pass\n",
    "\n",
    "    return num_entities, total_problema, total_teste, total_tratamento, total_anatomia\n",
    "\n",
    "\n",
    "def countEntities(path):\n",
    "    total_entities=0\n",
    "    total_problema=0\n",
    "    total_teste=0\n",
    "    total_tratamento=0\n",
    "    total_anatomia=0\n",
    "    \n",
    "    files=glob.glob(path+'*.ann')\n",
    "    #print(path,len(files))\n",
    "    \n",
    "    for i,file_path in enumerate(files):\n",
    "        ne, td, tr, tsk, tsy=getAnnotations(file_path)\n",
    "        total_entities+=ne\n",
    "        total_problema+=td\n",
    "        total_teste+=tr\n",
    "        total_tratamento+=tsk\n",
    "        total_anatomia+=tsy\n",
    "\n",
    "    return total_entities, total_problema, total_teste, total_tratamento, total_anatomia\n",
    "\n",
    "\n",
    "def countEntitiesDescontinuas(path):\n",
    "    total_entities=0\n",
    "    total_problema=0\n",
    "    total_teste=0\n",
    "    total_tratamento=0\n",
    "    total_anatomia=0\n",
    "    \n",
    "    files=glob.glob(path+'*.ann')\n",
    "    #print(path,len(files))\n",
    "    \n",
    "    for i,file_path in enumerate(files):\n",
    "        ne, td, tr, tsk, tsy=getAnnotationsDescontinuas(file_path)\n",
    "        total_entities+=ne\n",
    "        total_problema+=td\n",
    "        total_teste+=tr\n",
    "        total_tratamento+=tsk\n",
    "        total_anatomia+=tsy\n",
    "\n",
    "    return total_entities, total_problema, total_teste, total_tratamento, total_anatomia\n",
    "\n",
    "print('Entities,Problema,Teste, Tratamento, Anatomia')\n",
    "total_entities, total_problema, total_teste, total_tratamento, total_anatomia= countEntities(train_path)\n",
    "print(total_entities, total_problema, total_teste, total_tratamento, total_anatomia)\n",
    "\n",
    "total_entities, total_problema, total_teste, total_tratamento, total_anatomia = countEntities(test_path)\n",
    "print(total_entities, total_problema, total_teste, total_tratamento, total_anatomia)\n",
    "\n",
    "print('\\n--Descontinuas--\\n')\n",
    "print('Entities,Problema,Teste, Tratamento, Anatomia')\n",
    "total_entities, total_problema, total_teste, total_tratamento, total_anatomia= countEntitiesDescontinuas(train_path)\n",
    "print(total_entities, total_problema, total_teste, total_tratamento, total_anatomia)\n",
    "\n",
    "total_entities, total_problema, total_teste, total_tratamento, total_anatomia = countEntitiesDescontinuas(test_path)\n",
    "print(total_entities, total_problema, total_teste, total_tratamento, total_anatomia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "brat_statistics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
