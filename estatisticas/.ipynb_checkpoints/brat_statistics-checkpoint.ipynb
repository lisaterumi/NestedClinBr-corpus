{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clk8Dp2h-0QX"
   },
   "source": [
    "# Statistics\n",
    "This notebooks allows you to obtain some statistics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uLgJ6C4H2C_J",
    "outputId": "86cbdc95-f19a-4200-f7b2-db07bd517ed6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: ../corpus/brat/train/\n",
      "test: ../corpus/brat/test/\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount(\"/content/drive/\")\n",
    "\n",
    "#root='drive/My Drive/Colab Notebooks/nlp4rareNER/'\n",
    "#path=root+'data/goldstandard/'\n",
    "\n",
    "path = '../corpus/brat/'\n",
    "\n",
    "train_path=path+'train/'\n",
    "test_path=path+'test/'\n",
    "\n",
    "print(\"training:\", train_path)\n",
    "print(\"test:\", test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtMcXTN02VEQ"
   },
   "source": [
    "## Number of documents, sentences and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "he3lPxAE19gg",
    "outputId": "0d57d31e-75e2-4135-8522-fc9a1e4219fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents in training: 100\n",
      "number of documents in test: 26\n",
      "total of documents: 126\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "size_train=len(glob.glob(train_path+'*.txt'))\n",
    "print('number of documents in training:',size_train)\n",
    "size_test=len(glob.glob(test_path+'*.txt'))\n",
    "print('number of documents in test:',size_test)\n",
    "total=size_train+size_test\n",
    "print('total of documents:',total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n68iJmww3aG-"
   },
   "source": [
    "We will install Spacy, a library NLP, that helps us to tokenize the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PYnegGK92aSw",
    "outputId": "feb78a92-b1ce-46a0-8c2c-9cb7cd92973e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-13 10:01:45.717019: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n",
      "2022-08-13 10:01:45.717053: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-08-13 10:01:47.831223: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
      "2022-08-13 10:01:47.831256: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-13 10:01:47.835461: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: TerumiNB\n",
      "2022-08-13 10:01:47.835593: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: TerumiNB\n",
      "ERROR: Invalid requirement: '#install'\n",
      "WARNING: You are using pip version 20.3.3; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\lisat\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm #install spacy and download the model en_core_web_sm\n",
    "import spacy #NLP library for sentence segmentation and tokenization\n",
    "#nlp = spacy.load(\"pt_core_news_lg\")\n",
    "#!python -m spacy download pt_core_news_sm\n",
    "nlp = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "h2MsOgZcGsak"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences and tokens in training:  2273 17154\n",
      "Sentences and tokens in text:  693 5310\n"
     ]
    }
   ],
   "source": [
    "def getText(path_file):\n",
    "    \"\"\"returns the text of the file path_file\"\"\"\n",
    "    f=open(path_file,'r', encoding='utf-8')\n",
    "    text=f.read()\n",
    "    f.close()\n",
    "    return text\n",
    "\n",
    "def countTokens(path):\n",
    "    \"\"\"This functions takes a directory path where a dataset is stored, g\n",
    "    gets its txt files to parse them  by using Spacy \n",
    "    for obtaining the number of tokens and sentences in\n",
    "    the dataset\"\"\"\n",
    "    \n",
    "    total_sentences=0\n",
    "    total_tokens=0\n",
    "    files=glob.glob(path+'*.txt')\n",
    "    #print(path,len(files))\n",
    "    \n",
    "    for i,file_text in enumerate(files):\n",
    "        text=getText(file_text)\n",
    "        #print(file_text,text)\n",
    "\n",
    "        doc = nlp(text)\n",
    "        total_sentences+=len(list(doc.sents))\n",
    "        total_tokens+=len(doc)\n",
    "\n",
    "    return total_sentences, total_tokens\n",
    "\n",
    "sentences_train, tokens_train = countTokens(train_path)\n",
    "sentences_test, tokens_test = countTokens(test_path)\n",
    "\n",
    "print('Sentences and tokens in training: ', sentences_train, tokens_train)\n",
    "print('Sentences and tokens in text: ', sentences_test, tokens_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVhCbnN8DQEX"
   },
   "source": [
    "## Number of entities\n",
    "\n",
    "We now obtain the number of entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G3Tp5U7yALUp",
    "outputId": "6b2bc29d-a16b-47db-bd08-f2372ac83389"
   },
   "outputs": [],
   "source": [
    "def getAnnotations(path_file,allTypes=False):\n",
    "    \"\"\"returns a dataframe with its annotations\"\"\"\n",
    "    annotations=[]\n",
    "    try:  \n",
    "        f=open(path_file,'r')\n",
    "        annotations = f.readlines()       \n",
    "        f.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    num_entities=0\n",
    "    total_problema=0\n",
    "    total_teste=0\n",
    "    total_tratamento=0\n",
    "    total_anatomia=0\n",
    "\n",
    "    for ann in annotations:\n",
    "        if ann.startswith('T'):\n",
    "            idEnt=ann[:ann.index('\\t')]\n",
    "            typeEnt=ann[ann.index('\\t')+1:ann.index(' ')]\n",
    "            num_entities+=1\n",
    "\n",
    "            if typeEnt=='Problema':\n",
    "                if not allTypes:\n",
    "                    total_problema+=1\n",
    "                else:\n",
    "                    num_entities-=1\n",
    "            elif typeEnt=='Teste':\n",
    "                total_teste+=1\n",
    "            elif typeEnt=='Tratamento':\n",
    "                total_tratamento+=1\n",
    "            elif typeEnt=='Anatomia':\n",
    "                total_anatomia+=1\n",
    "            else:\n",
    "                #if allTypes==False\n",
    "                print('ERRO, typeEnt:', typeEnt)\n",
    "                pass\n",
    "\n",
    "    return num_entities, total_problema, total_teste, total_tratamento, total_anatomia\n",
    "\n",
    "def getAnnotationsDescontinuas(path_file,allTypes=False):\n",
    "    \"\"\"returns a dataframe with its annotations\"\"\"\n",
    "    annotations=[]\n",
    "    try:  \n",
    "        f=open(path_file,'r')\n",
    "        annotations = f.readlines()       \n",
    "        f.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    num_entities=0\n",
    "    total_problema=0\n",
    "    total_teste=0\n",
    "    total_tratamento=0\n",
    "    total_anatomia=0\n",
    "\n",
    "    for ann in annotations:\n",
    "        if ann.startswith('T'):\n",
    "            idEnt=ann[:ann.index('\\t')]\n",
    "            typeEnt=ann[ann.index('\\t')+1:ann.index(' ')]\n",
    "            listaAnn = ann.split('\\t')\n",
    "            indices=listaAnn[1]\n",
    "            if ';' in indices:\n",
    "                # ex T10\tProblema 244 252;283 306\tdispneia mdoeardos-leves esforço\n",
    "                num_entities+=1\n",
    "                if typeEnt=='Problema':\n",
    "                    total_problema+=1\n",
    "                elif typeEnt=='Teste':\n",
    "                    total_teste+=1\n",
    "                elif typeEnt=='Tratamento':\n",
    "                    total_tratamento+=1\n",
    "                elif typeEnt=='Anatomia':\n",
    "                    total_anatomia+=1\n",
    "                else:\n",
    "                    #if allTypes==False\n",
    "                    print('ERRO, typeEnt:', typeEnt)\n",
    "                    pass\n",
    "\n",
    "    return num_entities, total_problema, total_teste, total_tratamento, total_anatomia\n",
    "\n",
    "def getAnnotationsNested(path_file,allTypes=False):\n",
    "    \"\"\"returns a dataframe with its annotations\"\"\"\n",
    "    annotations=[]\n",
    "    try:  \n",
    "        f=open(path_file,'r')\n",
    "        annotations = f.readlines()       \n",
    "        f.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    num_entities=0\n",
    "    total_problema=0\n",
    "    total_teste=0\n",
    "    total_tratamento=0\n",
    "    total_anatomia=0\n",
    "\n",
    "    listaAllIndices = [] # [inicio, fim]\n",
    "    # lista com todos indices do arquivo de ann\n",
    "    #print('annotations:', annotations)\n",
    "    #print('path_file:', path_file)\n",
    "    for ann in annotations:\n",
    "        if ann.startswith('T'):\n",
    "            listaAnn = ann.split('\\t')\n",
    "            indices=listaAnn[1]\n",
    "            if ';' in indices:\n",
    "                indices = indices.split(';')\n",
    "                indice1=indices[0]\n",
    "                indice2=indices[1]                \n",
    "                indices1 = indice1.split()\n",
    "                inicio1, fim1 = indices1[1:3]\n",
    "                listaAllIndices.append([inicio1, fim1])\n",
    "                indices2 = indice2.split()\n",
    "                inicio2, fim2 = indices2[0:2]\n",
    "                listaAllIndices.append([inicio2, fim2])\n",
    "            else:\n",
    "                indices = indices.split()\n",
    "                inicio, fim = indices[1:3]\n",
    "                listaAllIndices.append([inicio, fim])\n",
    "            \n",
    "    for ann in annotations:\n",
    "        if ann.startswith('T'):\n",
    "            idEnt=ann[:ann.index('\\t')]\n",
    "            typeEnt=ann[ann.index('\\t')+1:ann.index(' ')]\n",
    "            listaAnn = ann.split('\\t')\n",
    "            indices=listaAnn[1]\n",
    "            # ex T1\tTratamento 43 57\tPlastia Mitral\n",
    "            #    T22\tAnatomia 51 57\tMitral\n",
    "            \n",
    "            if ';' in indices:\n",
    "                indices = indices.split(';')\n",
    "            else:\n",
    "                indices = indices.split()\n",
    "                inicio, fim = indices[1:3]\n",
    "                for indiceAll in listaAllIndices:\n",
    "                    inicioAll, fimAll = indiceAll\n",
    "                    if (int(inicioAll) <= int(fim) and int(inicioAll) >= int(inicio)) and not (int(inicioAll) == int(inicio) and int(fimAll) == int(fim)) : # nao pode ser ela mesma\n",
    "                        num_entities+=1\n",
    "                        if typeEnt=='Problema':\n",
    "                            total_problema+=1\n",
    "                            break\n",
    "                        elif typeEnt=='Teste':\n",
    "                            total_teste+=1\n",
    "                            break\n",
    "                        elif typeEnt=='Tratamento':\n",
    "                            total_tratamento+=1\n",
    "                            break\n",
    "                        elif typeEnt=='Anatomia':\n",
    "                            total_anatomia+=1\n",
    "                            break\n",
    "                        else:\n",
    "                            #if allTypes==False\n",
    "                            print('ERRO, typeEnt:', typeEnt)\n",
    "                            break\n",
    "                            \n",
    "\n",
    "    return num_entities, total_problema, total_teste, total_tratamento, total_anatomia\n",
    "\n",
    "def countEntities(path):\n",
    "    total_entities=0\n",
    "    total_problema=0\n",
    "    total_teste=0\n",
    "    total_tratamento=0\n",
    "    total_anatomia=0\n",
    "    \n",
    "    files=glob.glob(path+'*.ann')\n",
    "    #print(path,len(files))\n",
    "    \n",
    "    for i,file_path in enumerate(files):\n",
    "        ne, td, tr, tsk, tsy=getAnnotations(file_path)\n",
    "        total_entities+=ne\n",
    "        total_problema+=td\n",
    "        total_teste+=tr\n",
    "        total_tratamento+=tsk\n",
    "        total_anatomia+=tsy\n",
    "\n",
    "    return total_entities, total_problema, total_teste, total_tratamento, total_anatomia\n",
    "\n",
    "\n",
    "def countEntitiesDescontinuas(path):\n",
    "    total_entities=0\n",
    "    total_problema=0\n",
    "    total_teste=0\n",
    "    total_tratamento=0\n",
    "    total_anatomia=0\n",
    "    \n",
    "    files=glob.glob(path+'*.ann')\n",
    "    #print(path,len(files))\n",
    "    \n",
    "    for i,file_path in enumerate(files):\n",
    "        ne, td, tr, tsk, tsy=getAnnotationsDescontinuas(file_path)\n",
    "        total_entities+=ne\n",
    "        total_problema+=td\n",
    "        total_teste+=tr\n",
    "        total_tratamento+=tsk\n",
    "        total_anatomia+=tsy\n",
    "\n",
    "    return total_entities, total_problema, total_teste, total_tratamento, total_anatomia\n",
    "\n",
    "def countEntitiesNested(path):\n",
    "    total_entities=0\n",
    "    total_problema=0\n",
    "    total_teste=0\n",
    "    total_tratamento=0\n",
    "    total_anatomia=0\n",
    "    \n",
    "    files=glob.glob(path+'*.ann')\n",
    "    #print(path,len(files))\n",
    "    \n",
    "    for i,file_path in enumerate(files):\n",
    "        ne, td, tr, tsk, tsy=getAnnotationsNested(file_path)\n",
    "        total_entities+=ne\n",
    "        total_problema+=td\n",
    "        total_teste+=tr\n",
    "        total_tratamento+=tsk\n",
    "        total_anatomia+=tsy\n",
    "\n",
    "    return total_entities, total_problema, total_teste, total_tratamento, total_anatomia\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--Todas--\n",
      "\n",
      "Train:\n",
      "Entities,Problema,Teste, Tratamento, Anatomia\n",
      "2188 780 523 528 357\n",
      "Porcentagem:\n",
      "100 35.65 23.9 24.13 16\n",
      "\n",
      "Teste:\n",
      "Entities,Problema,Teste, Tratamento, Anatomia\n",
      "651 222 150 140 139\n",
      "Porcentagem:\n",
      "100 34.1 23.04 21.51 21\n",
      "\n",
      "--Descontinuas--\n",
      "\n",
      "Train:\n",
      "Entities,Problema,Teste, Tratamento, Anatomia\n",
      "69 63 0 4 2\n",
      "Porcentagem:\n",
      "100 91.3 0.0 5.8 3\n",
      "Porcentagem Geral:\n",
      "3.15 2.88 0.0 0.18 0\n",
      "\n",
      "Teste:\n",
      "Entities,Problema,Teste, Tratamento, Anatomia\n",
      "29 28 0 0 1\n",
      "Porcentagem:\n",
      "100 96.55 0.0 0.0 3\n",
      "Porcentagem Geral:\n",
      "4.45 4.3 0.0 0.0 0\n",
      "\n",
      "--Nested--\n",
      "\n",
      "Train:\n",
      "Entities,Problema,Teste, Tratamento, Anatomia\n",
      "227 167 7 31 22\n",
      "Porcentagem:\n",
      "100 73.57 3.08 13.66 10\n",
      "Porcentagem Geral:\n",
      "10.37 7.63 0.0 1.42 1\n",
      "\n",
      "Teste:\n",
      "Entities,Problema,Teste, Tratamento, Anatomia\n",
      "82 60 2 8 12\n",
      "Porcentagem:\n",
      "100 73.17 2.44 9.76 15\n",
      "Porcentagem Geral:\n",
      "4.45 4.3 0.0 0.0 0\n"
     ]
    }
   ],
   "source": [
    "print('\\n--Todas--\\n')\n",
    "print('Train:')\n",
    "print('Entities,Problema,Teste, Tratamento, Anatomia')\n",
    "total_entitiesTrain, total_problemaTrain, total_testeTrain, total_tratamentoTrain, total_anatomiaTrain= countEntities(train_path)\n",
    "print(total_entitiesTrain, total_problemaTrain, total_testeTrain, total_tratamentoTrain, total_anatomiaTrain)\n",
    "print('Porcentagem:')\n",
    "print(100, round((total_problemaTrain/total_entitiesTrain*100), 2), round(total_testeTrain/total_entitiesTrain*100, 2), round(total_tratamentoTrain/total_entitiesTrain*100, 2), round(total_anatomiaTrain/total_entitiesTrain*100))\n",
    "\n",
    "print('\\nTeste:')\n",
    "total_entitiesTest, total_problemaTest, total_testeTest, total_tratamentoTest, total_anatomiaTest = countEntities(test_path)\n",
    "print('Entities,Problema,Teste, Tratamento, Anatomia')\n",
    "print(total_entitiesTest, total_problemaTest, total_testeTest, total_tratamentoTest, total_anatomiaTest)\n",
    "print('Porcentagem:')\n",
    "print(100, round((total_problemaTest/total_entitiesTest*100), 2), round(total_testeTest/total_entitiesTest*100, 2), round(total_tratamentoTest/total_entitiesTest*100, 2), round(total_anatomiaTest/total_entitiesTest*100))\n",
    "\n",
    "print('\\n--Descontinuas--\\n')\n",
    "print('Train:')\n",
    "print('Entities,Problema,Teste, Tratamento, Anatomia')\n",
    "total_entitiesTrainDesc, total_problemaTrainDesc, total_testeTrainDesc, total_tratamentoTrainDesc, total_anatomiaTrainDesc= countEntitiesDescontinuas(train_path)\n",
    "print(total_entitiesTrainDesc, total_problemaTrainDesc, total_testeTrainDesc, total_tratamentoTrainDesc, total_anatomiaTrainDesc)\n",
    "print('Porcentagem:')\n",
    "print(100, round((total_problemaTrainDesc/total_entitiesTrainDesc*100), 2), round(total_testeTrainDesc/total_entitiesTrainDesc*100, 2), round(total_tratamentoTrainDesc/total_entitiesTrainDesc*100, 2), round(total_anatomiaTrainDesc/total_entitiesTrainDesc*100))\n",
    "print('Porcentagem Geral:')\n",
    "print(round((total_entitiesTrainDesc/total_entitiesTrain*100), 2), round((total_problemaTrainDesc/total_entitiesTrain*100), 2), round(total_testeTrainDesc/total_entitiesTrain*100, 2), round(total_tratamentoTrainDesc/total_entitiesTrain*100, 2), round(total_anatomiaTrainDesc/total_entitiesTrain*100))\n",
    "\n",
    "print('\\nTeste:')\n",
    "total_entitiesTestDesc, total_problemaTestDesc, total_testeTestDesc, total_tratamentoTestDesc, total_anatomiaTestDesc = countEntitiesDescontinuas(test_path)\n",
    "print('Entities,Problema,Teste, Tratamento, Anatomia')\n",
    "print(total_entitiesTestDesc, total_problemaTestDesc, total_testeTestDesc, total_tratamentoTestDesc, total_anatomiaTestDesc)\n",
    "print('Porcentagem:')\n",
    "print(100, round((total_problemaTestDesc/total_entitiesTestDesc*100), 2), round(total_testeTestDesc/total_entitiesTestDesc*100, 2), round(total_tratamentoTestDesc/total_entitiesTestDesc*100, 2), round(total_anatomiaTestDesc/total_entitiesTestDesc*100))\n",
    "print('Porcentagem Geral:')\n",
    "print(round((total_entitiesTestDesc/total_entitiesTest*100), 2), round((total_problemaTestDesc/total_entitiesTest*100), 2), round(total_testeTestDesc/total_entitiesTest*100, 2), round(total_tratamentoTestDesc/total_entitiesTest*100, 2), round(total_anatomiaTestDesc/total_entitiesTest*100))\n",
    "\n",
    "print('\\n--Nested--\\n')\n",
    "print('Train:')\n",
    "print('Entities,Problema,Teste, Tratamento, Anatomia')\n",
    "total_entitiesTrainNested, total_problemaTrainNested, total_testeTrainNested, total_tratamentoTrainNested, total_anatomiaTrainNested= countEntitiesNested(train_path)\n",
    "print(total_entitiesTrainNested, total_problemaTrainNested, total_testeTrainNested, total_tratamentoTrainNested, total_anatomiaTrainNested)\n",
    "print('Porcentagem:')\n",
    "print(100, round((total_problemaTrainNested/total_entitiesTrainNested*100), 2), round(total_testeTrainNested/total_entitiesTrainNested*100, 2), round(total_tratamentoTrainNested/total_entitiesTrainNested*100, 2), round(total_anatomiaTrainNested/total_entitiesTrainNested*100))\n",
    "print('Porcentagem Geral:')\n",
    "print(round((total_entitiesTrainNested/total_entitiesTrain*100), 2), round((total_problemaTrainNested/total_entitiesTrain*100), 2), round(total_testeTrainDesc/total_entitiesTrain*100, 2), round(total_tratamentoTrainNested/total_entitiesTrain*100, 2), round(total_anatomiaTrainNested/total_entitiesTrain*100))\n",
    "\n",
    "print('\\nTeste:')\n",
    "total_entitiesTestNested, total_problemaTestNested, total_testeTestNested, total_tratamentoTestNested, total_anatomiaTestNested = countEntitiesNested(test_path)\n",
    "print('Entities,Problema,Teste, Tratamento, Anatomia')\n",
    "print(total_entitiesTestNested, total_problemaTestNested, total_testeTestNested, total_tratamentoTestNested, total_anatomiaTestNested)\n",
    "print('Porcentagem:')\n",
    "print(100, round((total_problemaTestNested/total_entitiesTestNested*100), 2), round(total_testeTestNested/total_entitiesTestNested*100, 2), round(total_tratamentoTestNested/total_entitiesTestNested*100, 2), round(total_anatomiaTestNested/total_entitiesTestNested*100))\n",
    "print('Porcentagem Geral:')\n",
    "print(round((total_entitiesTestNested/total_entitiesTest*100), 2), round((total_problemaTestNested/total_entitiesTest*100), 2), round(total_testeTestDesc/total_entitiesTest*100, 2), round(total_tratamentoTestNested/total_entitiesTest*100, 2), round(total_anatomiaTestNested/total_entitiesTest*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "brat_statistics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
